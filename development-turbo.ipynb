{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05afaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, emoji\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "# import ipywidgets as widgets\n",
    "import openai\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "\n",
    "openai.api_key = os.getenv(\"YOUR_OPEN_AI_API_KEY\")\n",
    "\n",
    "from prompt_template import create_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515813ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this block of code I am cleaning the data as khouloud did\n",
    "'''\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def preprocess_tweet_text(self, text):\n",
    "        text = re.sub(r'#([^ ]*)', r'\\1', text)\n",
    "        text = re.sub(r'https.*[^ ]', 'URL', text)\n",
    "        text = re.sub(r'http.*[^ ]', 'URL', text)\n",
    "        text = emoji.demojize(text)\n",
    "        text = re.sub(r'(:.*?:)', r' \\1 ', text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def preprocess_dataframe(self, df):\n",
    "        df['text'] = df['text'].apply(self.preprocess_tweet_text)\n",
    "        return df\n",
    "\n",
    "    \n",
    "# Instantiate a TextProcessor object\n",
    "text_processor = TextProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49839667-e4d6-4665-aecb-7b8c2501ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reading the csv file in pandas dataframe and converting the label\n",
    "column to string column\n",
    "\n",
    "Reason of choosing that string label:\n",
    "I believe there should be a strong correlation between what the label is \n",
    "when we give it to the GPT model, it must have some prior information about \n",
    "the label.\n",
    "\n",
    "To Test this I have to repeat the experiment where I will ask the model to \n",
    "label as 0 or 1 (not hate speech and hate speech respectively)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "df_hateval_test= pd.read_csv(expanded_path_hateval_test_csv)\n",
    "print(\"==Before Converting Numerical Label to String===\")\n",
    "print(df_hateval_test.head())\n",
    "print(\"==After Converting Numerical Label to String===\")\n",
    "# %debug\n",
    "# Define a dictionary to map values\n",
    "mapping = {1: \"Hate speech\", 0: \"Not hate speech\"}\n",
    "df_hateval_test['HS'] = df_hateval_test['HS'].map(mapping)\n",
    "print(df_hateval_test.head())\n",
    "print(\"hello darkness my old friend\")\n",
    "print(f\"Lenght of the dataframe: {len(df_hateval_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1304a-41e2-4102-9589-1e118813e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF = df_hateval_test\n",
    "# folds = KFold(n_splits=5)\n",
    "\n",
    "# creating a dictionary to save the classification report.\n",
    "cv_dictionary = dict()\n",
    "\n",
    "# it means the same as run and it is used  for the purpose\n",
    "# for printing and saving all the importante report w.r.t the current run\n",
    "fold = 2\n",
    "\n",
    "# defines how many iteration you want to do for the current experiment.\n",
    "total_iteration = 3\n",
    "\n",
    "for i in range(total_iteration):\n",
    "    \n",
    "    fold += 1\n",
    "    print(f\"===========RUN: {fold} starting===========\")\n",
    "    # Cleaning the test dataset\n",
    "    df_test = text_processor.preprocess_dataframe(df_hateval_test)\n",
    "    \n",
    "    # this will take only 5 instances from the test set and perform the model inference\n",
    "    # df_test = df_test.loc[:5-1]\n",
    "\n",
    "    '''\n",
    "    getting the batches of batches as it was before\n",
    "    '''\n",
    "    # create an empty list to store the index values\n",
    "    batches_of_batches = []\n",
    "\n",
    "    # loop over the dataframe using iterrows() method\n",
    "    for index, row in df_test.iterrows():\n",
    "        # append the index value to the list\n",
    "        batches_of_batches.append([index])\n",
    "        \n",
    "\n",
    "    if not os.path.exists(f'response_hatespeech_run_{fold}.csv'):\n",
    "        with open(f\"response_hatespeech_run_{fold}.csv\", \"a\", newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Input\", \"Complete Response\", \"Model Output\", \"Date and Time\"])\n",
    "\n",
    "    if not os.path.exists(f'groundtruth_hatespeech_run_{fold}.csv'):\n",
    "        with open(f\"groundtruth_hatespeech_run_{fold}.csv\", \"a\", newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"test-instance-index\",\"groundtruth-text\", \"groundtruth-label\"])\n",
    "\n",
    "\n",
    "    # Go through each batch (it has n instances)\n",
    "    for batch_number, i in enumerate(batches_of_batches):\n",
    "\n",
    "        text_list_gold = list()\n",
    "\n",
    "\n",
    "        # loop through each instances within a batch\n",
    "        for k in i:\n",
    "\n",
    "            # k = index\n",
    "\n",
    "            text_list_gold.append(df_test.loc[k]['text'])\n",
    "            data = [k, df_test.loc[k]['text'], df_test.loc[k]['HS']]\n",
    "\n",
    "            # Open the CSV file for writing the ground truth\n",
    "            with open(f\"groundtruth_hatespeech_run_{fold}.csv\", \"a\", newline='', encoding=\"utf-8\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                # Write the data rows\n",
    "                writer.writerow(data)\n",
    "\n",
    "        # Below creating the final template, containing the appended text and the examples that \n",
    "        # are to be classified by the model\n",
    "\n",
    "        # text_list_gold has the text that I want to classify with the model\n",
    "        for text in text_list_gold:\n",
    "            # template += f\"\\nText:{text}\\nLabel:\"\n",
    "            template = create_prompt(text, fourshot=True)\n",
    "        # print(template)\n",
    "\n",
    "        # break\n",
    "\n",
    "        # Note: \n",
    "\n",
    "        '''\n",
    "        In all experiments throughout this paper, for our calls to GPT-3,\n",
    "        we Players_(film)Players_(film)set the temperature to 0.0 and the top_P to 1.0. 1 These settings\n",
    "        serve two purposes. First, they ensure reproducibility, specifically\n",
    "        that GPT-3 will always have the same response given the prompt\n",
    "        we pass it. Second, they minimize the risk that GPT-3 will wander off topic or hallucinate.\n",
    "\n",
    "        Paper: Can GPT-3 Perform Statutory Reasoning?\n",
    "        Link: https://arxiv.org/pdf/2302.06100.pdf\n",
    "\n",
    "        '''\n",
    "\n",
    "        retry_limit = 10  # Maximum number of retries\n",
    "\n",
    "        try_count = 0\n",
    "\n",
    "        while try_count < retry_limit:\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You must reply with either 'Hate speech' or 'Not hate speech' based solely on the linguistic content of the text that you will be asked to classify.\"},\n",
    "                        {\"role\": \"user\", \"content\": template}\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                )\n",
    "\n",
    "                # If the response is successful, break out of the loop\n",
    "                break\n",
    "\n",
    "            except openai.error.RateLimitError as e:\n",
    "                print(\"Rate limit exceeded. Waiting for 3 minute before retrying...\")\n",
    "                time.sleep(180)  # Wait for 3 minute (adjust as needed)\n",
    "                try_count += 1  # Increment the try count and retry\n",
    "\n",
    "            except Exception as e:\n",
    "                # catch me if you can i.e. catch everything. I am determined to get the model output no matter what - we live only once right?\n",
    "                print(f\"This Exception: {e}\")\n",
    "                print(f\"For this batch, we got the error: {batch_number}\")\n",
    "                print(f\"These are the instances that the model could not create a completion for: {i}\")\n",
    "                print(f\"For this exception {e}, Waiting for 5 minute before retrying...\")\n",
    "                time.sleep(300)  # Wait for 3 minute (adjust as needed)\n",
    "                try_count += 1\n",
    "\n",
    "        if try_count == retry_limit:\n",
    "            print(\"Reached the maximum number of retries. Unable to get a successful response.\\nTherefore using the previous label as the current label for the test text\")\n",
    "\n",
    "        # print(\"something\")\n",
    "        # saving some data\n",
    "        now = datetime.datetime.now()\n",
    "        data = [template, response, response['choices'][0]['message']['content'], now.strftime(\"%Y-%m-%d %H:%M:%S\")]\n",
    "        # Open the CSV file for writing\n",
    "        with open(f\"response_hatespeech_run_{fold}.csv\", \"a\", newline='',encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write the data rows\n",
    "            writer.writerow(data)\n",
    "        \n",
    "        label = response['choices'][0]['message']['content']\n",
    "        # i[0] is the instance index that is coming from batches_of_batches\n",
    "        instance_label = [(i[0], label)]\n",
    "        try:\n",
    "            with open(f\"modeloutput_hatespeech_run_{fold}.csv\", 'x') as file:\n",
    "                # Create a new Dataframe with the label\n",
    "                df = pd.DataFrame(instance_label, columns=[\"test-instance-index\",\"gpt-label\"])\n",
    "                # Write the DataFrame to the csv file\n",
    "                df.to_csv(f\"modeloutput_hatespeech_run_{fold}.csv\", index=False)\n",
    "        except FileExistsError:\n",
    "            # If the file already exists, append the data to it\n",
    "            with open(f\"modeloutput_hatespeech_run_{fold}.csv\", 'a', encoding=\"utf-8\") as file:\n",
    "                # Create a new DataFrame with the data\n",
    "                df = pd.DataFrame(instance_label, columns=[\"test-instance-index\",\"gpt-label\"])\n",
    "                # Append the DataFrame to the csv file\n",
    "                df.to_csv(file, header=False, index=False)\n",
    "\n",
    "    \n",
    "    # reading the models output and groundtruth for creating the classificaion report\n",
    "\n",
    "    ground_truth = pd.read_csv(f\"groundtruth_hatespeech_run_{fold}.csv\")\n",
    "    model_output = pd.read_csv(f\"modeloutput_hatespeech_run_{fold}.csv\")\n",
    "    \n",
    "    # Ground truth data\n",
    "    y_true = ground_truth[\"groundtruth-label\"]\n",
    "\n",
    "    # Model output data\n",
    "    y_pred = model_output[\"gpt-label\"]\n",
    "\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    \n",
    "    report_dictionary = classification_report(y_true, y_pred, output_dict=True)\n",
    "    #saving each classificaiton report for the current run\n",
    "    cv_dictionary[f'run_{fold}'] = report_dictionary\n",
    "    \n",
    "    print(report)\n",
    "    print(\"\\n\")\n",
    "    print(f\"=========Run: {fold} Finish ==============\\n\")\n",
    "    print(f\"\\n=====Sleeping for 5 minute before going to the next run=====\\n\")\n",
    "    time.sleep(300)\n",
    "\n",
    "\n",
    "# saving the complete report for each run \n",
    "with open(\"cv_run_3_4_5_report.json\", \"w\") as f:\n",
    "          json.dump(cv_dictionary, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575260ff",
   "metadata": {},
   "source": [
    "How to handle rate limits: \n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd823bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0527c9-9e9d-4fd6-9504-d7a1c3797589",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For string output\n",
    "'''\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        # {\"role\": \"system\", \"content\": \"You must reply with either only 'Hate speech' or 'Not hate speech' based solely on the text that you will be asked to classify\"}\n",
    "        {\"role\": \"user\", \"content\": template_temp}\n",
    "\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
